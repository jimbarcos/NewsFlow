#!/usr/bin/env python3
"""
Universal News Scraper for Inquirer, Business Mirror, and Philstar
Scrapes all sources, saves Excel files, and uploads to Azure Blob Storage
Optimized for GitHub Actions workflow with enhanced error handling
"""

import os
import sys
import pandas as pd
from datetime import datetime
import importlib
from dotenv import load_dotenv

# Load environment variables from .env file (for local development)
load_dotenv()

def import_scraper(module_name, function_name):
    """Import scraper functions with error handling"""
    try:
        module = importlib.import_module(module_name)
        return getattr(module, function_name)
    except Exception as e:
        print(f"‚ùå Error importing {module_name}.{function_name}: {e}")
        return None

def validate_azure_environment():
    """Validate Azure environment configuration for both local and CI environments"""
    azure_conn = os.getenv('AZURE_CONNECTION_STRING')
    azure_container = os.getenv('AZURE_CONTAINER_NAME')
    
    if not azure_conn:
        print("‚ùå AZURE_CONNECTION_STRING environment variable not found")
        print("üí° For local development: Check your .env file")
        print("üí° For GitHub Actions: Check repository secrets")
        return False
    
    if not azure_container:
        print("‚ùå AZURE_CONTAINER_NAME environment variable not found")
        print("üí° For local development: Check your .env file")
        print("üí° For GitHub Actions: Check repository secrets")
        return False
    
    print("‚úÖ Azure Blob Storage configuration validated")
    print(f"üì¶ Container: {azure_container}")
    return True

def main():
    """Main orchestrator function with enhanced error handling for GitHub Actions"""
    print("ü§ñ Universal News Scraper - GitHub Actions Optimized")
    print(f"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Validate Azure environment first
    if not validate_azure_environment():
        print("‚ùå Azure environment validation failed - cannot proceed")
        sys.exit(1)
    
    # Import enhanced scrapers with dynamic date filtering and improved extraction
    print("\nüì∞ Importing news scraper modules...")
    
    # Import all three scrapers
    inquirer_scraper = import_scraper('scrape_inquirer', 'scrape_inquirer_news')
    inquirer_upload = import_scraper('scrape_inquirer', 'upload_to_azure_blob')
    
    businessmirror_scraper = import_scraper('scrape_businessmirror_fixed', 'scrape_businessmirror_news')
    businessmirror_upload = import_scraper('scrape_businessmirror_fixed', 'upload_to_azure_blob')
    
    philstar_scraper = import_scraper('scrape_philstar_improved', 'scrape_philstar_news')
    philstar_upload = import_scraper('scrape_philstar_improved', 'upload_to_azure_blob')
    
    # Check if all imports were successful
    missing_functions = []
    if not inquirer_scraper: missing_functions.append("inquirer_scraper")
    if not inquirer_upload: missing_functions.append("inquirer_upload")
    if not businessmirror_scraper: missing_functions.append("businessmirror_scraper")
    if not businessmirror_upload: missing_functions.append("businessmirror_upload")
    if not philstar_scraper: missing_functions.append("philstar_scraper")
    if not philstar_upload: missing_functions.append("philstar_upload")
    
    if missing_functions:
        print(f"‚ùå Could not import required functions: {', '.join(missing_functions)}")
        print("üí° Ensure all scraper files are present and contain the required functions")
        sys.exit(1)
    
    print("‚úÖ All scraper modules imported successfully")
    print("üöÄ Enhanced Features:")
    print("   ‚Ä¢ Dynamic date filtering (today & yesterday)")
    print("   ‚Ä¢ Enhanced anti-bot measures with modern user agents")
    print("   ‚Ä¢ Retry logic for Azure uploads")
    print("   ‚Ä¢ Proper error handling for GitHub Actions")

    # Track success/failure for exit code
    scraping_errors = 0

    # Scrape Inquirer
    print("\n==============================")
    print("üîç Scraping Inquirer Business News...")
    try:
        inquirer_news = inquirer_scraper()
        if inquirer_news:
            import pandas as pd
            df_inq = pd.DataFrame(inquirer_news)
            inq_filename = "inquirer_news.xlsx"
            df_inq.to_excel(inq_filename, index=False)
            # Add Excel table named 'NewsTable'
            try:
                from openpyxl import load_workbook
                from openpyxl.worksheet.table import Table, TableStyleInfo
                wb = load_workbook(inq_filename)
                ws = wb.active
                nrows = ws.max_row
                ncols = ws.max_column
                col_letters = ws.cell(row=1, column=ncols).column_letter
                table_ref = f"A1:{col_letters}{nrows}"
                table = Table(displayName="NewsTable", ref=table_ref)
                style = TableStyleInfo(name="TableStyleMedium9", showFirstColumn=False,
                                       showLastColumn=False, showRowStripes=True, showColumnStripes=False)
                table.tableStyleInfo = style
                ws.add_table(table)
                wb.save(inq_filename)
                print(f"üìä Saved {len(df_inq)} Inquirer news items to {inq_filename} (with table 'NewsTable')")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not add Excel table to Inquirer file: {e}")
                print(f"üìä Saved {len(df_inq)} Inquirer news items to {inq_filename}")
            
            print("‚òÅÔ∏è Uploading Inquirer news to Azure...")
            upload_success = inquirer_upload(inq_filename, inq_filename)
            if not upload_success:
                print("‚ùå Failed to upload Inquirer news to Azure")
                scraping_errors += 1
            else:
                print("‚úÖ Inquirer news uploaded successfully")
        else:
            print("‚ùå No Inquirer news found.")
            scraping_errors += 1
    except Exception as e:
        print(f"‚ùå Inquirer scraping failed: {str(e)}")
        scraping_errors += 1

    # Scrape Business Mirror
    print("\n==============================")
    print("üîç Scraping Business Mirror News...")
    try:
        # Check if file is locked before proceeding
        bm_filename = "businessmirror_news.xlsx"
        if os.path.exists(bm_filename):
            try:
                # Test if we can access the file
                with open(bm_filename, 'r+b') as test_file:
                    pass
            except PermissionError:
                print(f"‚ö†Ô∏è  File {bm_filename} is currently locked. Please close any Excel applications and try again.")
                print("   Skipping Business Mirror scraping to continue with other sources...")
                bm_news = None
            except:
                print(f"   Removing existing file: {bm_filename}")
                os.remove(bm_filename)
        
        if 'bm_news' not in locals() or bm_news is None:
            bm_news = businessmirror_scraper()
        
        if bm_news:
            import pandas as pd
            df_bm = pd.DataFrame(bm_news)
            df_bm.to_excel(bm_filename, index=False)
            # Add Excel table named 'NewsTable1'
            try:
                from openpyxl import load_workbook
                from openpyxl.worksheet.table import Table, TableStyleInfo
                wb = load_workbook(bm_filename)
                ws = wb.active
                nrows = ws.max_row
                ncols = ws.max_column
                col_letters = ws.cell(row=1, column=ncols).column_letter
                table_ref = f"A1:{col_letters}{nrows}"
                table = Table(displayName="NewsTable1", ref=table_ref)
                style = TableStyleInfo(name="TableStyleMedium9", showFirstColumn=False,
                                       showLastColumn=False, showRowStripes=True, showColumnStripes=False)
                table.tableStyleInfo = style
                ws.add_table(table)
                wb.save(bm_filename)
                print(f"üìä Saved {len(df_bm)} Business Mirror news items to {bm_filename} (with table 'NewsTable1')")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not add Excel table to Business Mirror file: {e}")
                print(f"üìä Saved {len(df_bm)} Business Mirror news items to {bm_filename}")
            
            print("‚òÅÔ∏è Uploading Business Mirror news to Azure...")
            upload_success = businessmirror_upload(bm_filename, bm_filename)
            if not upload_success:
                print("‚ùå Failed to upload Business Mirror news to Azure")
                scraping_errors += 1
            else:
                print("‚úÖ Business Mirror news uploaded successfully")
        else:
            print("‚ùå No Business Mirror news found.")
            scraping_errors += 1
    except Exception as e:
        print(f"‚ùå Business Mirror scraping failed: {str(e)}")
        if "Permission denied" in str(e):
            print("   üí° Suggestion: Close any Excel files and ensure no applications are using the output file")
        scraping_errors += 1

    # Scrape Philstar
    print("\n==============================")
    print("üîç Scraping Philstar Business News...")
    try:
        philstar_news = philstar_scraper()
        if philstar_news:
            import pandas as pd
            df_philstar = pd.DataFrame(philstar_news)
            philstar_filename = "philstar_news.xlsx"
            df_philstar.to_excel(philstar_filename, index=False)
            # Add Excel table named 'NewsTable2'
            try:
                from openpyxl import load_workbook
                from openpyxl.worksheet.table import Table, TableStyleInfo
                wb = load_workbook(philstar_filename)
                ws = wb.active
                nrows = ws.max_row
                ncols = ws.max_column
                col_letters = ws.cell(row=1, column=ncols).column_letter
                table_ref = f"A1:{col_letters}{nrows}"
                table = Table(displayName="NewsTable2", ref=table_ref)
                style = TableStyleInfo(name="TableStyleMedium9", showFirstColumn=False,
                                       showLastColumn=False, showRowStripes=True, showColumnStripes=False)
                table.tableStyleInfo = style
                ws.add_table(table)
                wb.save(philstar_filename)
                print(f"üìä Saved {len(df_philstar)} Philstar news items to {philstar_filename} (with table 'NewsTable2')")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not add Excel table to Philstar file: {e}")
                print(f"üìä Saved {len(df_philstar)} Philstar news items to {philstar_filename}")
            
            print("‚òÅÔ∏è Uploading Philstar news to Azure...")
            upload_success = philstar_upload(philstar_filename, philstar_filename)
            if not upload_success:
                print("‚ùå Failed to upload Philstar news to Azure")
                scraping_errors += 1
            else:
                print("‚úÖ Philstar news uploaded successfully")
        else:
            print("‚ùå No Philstar news found.")
            scraping_errors += 1
    except Exception as e:
        print(f"‚ùå Philstar scraping failed: {str(e)}")
        scraping_errors += 1

    # Final status and exit code
    print("\n==============================")
    if scraping_errors == 0:
        print("‚úÖ Enhanced Universal News Scraping Complete!")
        print("üéØ All improvements implemented:")
        print("   ‚úì Dynamic date filtering (adapts to any date)")
        print("   ‚úì Business Mirror: More articles + proper categories")
        print("   ‚úì Inquirer: Better date filtering accuracy")
        print("   ‚úì Philstar: Infinite scroll simulation")
        print("   ‚úì Enhanced error handling for GitHub Actions")
        print("   ‚úì Retry logic for Azure uploads")
        print(f"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print(f"‚ö†Ô∏è Universal News Scraping completed with {scraping_errors} error(s)")
        print("üí° Check the logs above for specific error details")
        print(f"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        sys.exit(1)  # Exit with error code for GitHub Actions

if __name__ == "__main__":
    main()
